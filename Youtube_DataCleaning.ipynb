{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f4ba0-c842-4077-9a61-d7612bf0210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßº Cleaning video descriptions...\n",
      "üßº Cleaning comment text...\n",
      "üßº Cleaning reply text...\n",
      "üßº Cleaning edges data...\n",
      "\n",
      "‚úÖ Preprocessing complete. Cleaned files saved:\n",
      "   - videos_cleaned_text.csv\n",
      "   - comments_cleaned_text.csv\n",
      "   - replies_cleaned_text.csv\n",
      "   - edges_cleaned.csv\n",
      "\n",
      "üìä Rows after cleaning:\n",
      "Videos:   981\n",
      "Comments: 78785\n",
      "Replies:  13883\n",
      "Edges:    3581\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# === Clean Text Function ===\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# === Load Data ===\n",
    "videos = pd.read_csv('videos_combined.csv')\n",
    "comments = pd.read_csv('comments_combined.csv')\n",
    "replies = pd.read_csv('replies_combined.csv')\n",
    "edges = pd.read_csv('edges_combined.csv')\n",
    "\n",
    "# === Drop Duplicates ===\n",
    "videos = videos.drop_duplicates(subset=['video_id'])\n",
    "comments = comments.drop_duplicates()\n",
    "replies = replies.drop_duplicates()\n",
    "edges = edges.drop_duplicates()\n",
    "\n",
    "# === Drop Rows with Essential Nulls ===\n",
    "videos = videos.dropna(subset=['description'])\n",
    "comments = comments.dropna(subset=['text', 'author'])\n",
    "replies = replies.dropna(subset=['text', 'author'])\n",
    "edges = edges.dropna()\n",
    "\n",
    "# === Convert Date Columns to Datetime ===\n",
    "videos['published'] = pd.to_datetime(videos['published'], errors='coerce')\n",
    "comments['published_at'] = pd.to_datetime(comments['published_at'], errors='coerce')\n",
    "replies['published_at'] = pd.to_datetime(replies['published_at'], errors='coerce')\n",
    "\n",
    "# === Clean Text Columns ===\n",
    "print(\"Cleaning video descriptions...\")\n",
    "videos['cleaned_description'] = videos['description'].apply(clean_text)\n",
    "\n",
    "print(\" Cleaning comment text...\")\n",
    "comments['cleaned_text'] = comments['text'].apply(clean_text)\n",
    "\n",
    "print(\" Cleaning reply text...\")\n",
    "replies['cleaned_text'] = replies['text'].apply(clean_text)\n",
    "\n",
    "# === Clean Tags, Mentions, Hashtags in videos.csv ===\n",
    "for col in ['tags', 'mentions', 'hashtags']:\n",
    "    if col in videos.columns:\n",
    "        videos[col] = (\n",
    "            videos[col].fillna('')\n",
    "            .astype(str)\n",
    "            .str.lower()\n",
    "            .str.replace(r'[^a-z0-9, ]', '', regex=True)\n",
    "        )\n",
    "\n",
    "# === Clean edges.csv columns ===\n",
    "print(\"üßº Cleaning edges data...\")\n",
    "edges['source_channelId'] = edges['source_channelId'].astype(str).str.strip().str.lower()\n",
    "edges['target_channelId'] = edges['target_channelId'].astype(str).str.strip().str.lower()\n",
    "edges['video_ids'] = edges['video_ids'].astype(str).str.strip()\n",
    "edges['mention_count'] = pd.to_numeric(edges['mention_count'], errors='coerce').fillna(0).astype(int)\n",
    "edges['total_views'] = pd.to_numeric(edges['total_views'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# === Drop Empty Cleaned Rows ===\n",
    "videos = videos[videos['cleaned_description'].str.strip() != '']\n",
    "comments = comments[comments['cleaned_text'].str.strip() != '']\n",
    "replies = replies[replies['cleaned_text'].str.strip() != '']\n",
    "\n",
    "# === Save Cleaned Files ===\n",
    "videos.to_csv('videos_cleaned_text.csv', index=False)\n",
    "comments.to_csv('comments_cleaned_text.csv', index=False)\n",
    "replies.to_csv('replies_cleaned_text.csv', index=False)\n",
    "edges.to_csv('edges_cleaned.csv', index=False)\n",
    "\n",
    "print(\"\\n Preprocessing complete. Cleaned files saved:\")\n",
    "print(\"   - videos_cleaned_text.csv\")\n",
    "print(\"   - comments_cleaned_text.csv\")\n",
    "print(\"   - replies_cleaned_text.csv\")\n",
    "print(\"   - edges_cleaned.csv\")\n",
    "\n",
    "print(f\"\\nRows after cleaning:\")\n",
    "print(f\"Videos:   {len(videos)}\")\n",
    "print(f\"Comments: {len(comments)}\")\n",
    "print(f\"Replies:  {len(replies)}\")\n",
    "print(f\"Edges:    {len(edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd0850-3600-45ab-a4c9-cf1cff61f202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Column names in videos_cleaned_text.csv:\n",
      "['video_id', 'title', 'channel', 'channelId', 'published', 'description', 'tags', 'mentions', 'hashtags', 'views', 'source_folder', 'cleaned_description']\n",
      "\n",
      "üìÅ Column names in comments_cleaned_text.csv:\n",
      "['comment_id', 'video_id', 'author', 'published_at', 'text', 'source_folder', 'cleaned_text']\n",
      "\n",
      "üìÅ Column names in replies_cleaned_text.csv:\n",
      "['video_id', 'in_reply_to', 'parent_author', 'author', 'published_at', 'text', 'source_folder', 'cleaned_text']\n",
      "\n",
      "üìÅ Column names in edges_cleaned.csv:\n",
      "['source_channelId', 'target_channelId', 'mention_count', 'total_views', 'video_ids', 'source_channelId_name', 'target_channelId_name', 'source_folder']\n"
     ]
    }
   ],
   "source": [
    "# === Print Column Names ===\n",
    "print(\"Column names in videos_cleaned_text.csv:\")\n",
    "print(videos.columns.tolist())\n",
    "\n",
    "print(\"\\nColumn names in comments_cleaned_text.csv:\")\n",
    "print(comments.columns.tolist())\n",
    "\n",
    "print(\"\\nColumn names in replies_cleaned_text.csv:\")\n",
    "print(replies.columns.tolist())\n",
    "\n",
    "print(\"\\nColumn names in edges_cleaned.csv:\")\n",
    "print(edges.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abea0a6-0fc5-4794-852a-f937ad77950e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
