{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ab76ed0-2458-42c4-9d28-e799ae03cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b56045-6782-4d79-9313-d168f14bce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CONFIG: API Keys ---\n",
    "API_KEYS = [\n",
    "            \"AIzaSyDPSqJudfBLzj6Z-izKUN6DmdaSPC8WFpg\",\n",
    "            \"AIzaSyDF_0DC2m-n1oQlhvAsJWsa2p2LbpVhto4\",\n",
    "            \"AIzaSyBY-hES9Xy0gynLTZpNeAcp14Xz_ldkQ2Q\",\n",
    "            \"AIzaSyDB5__J5edGc6TRGDmaKZDG_KGahXXLS74\",\n",
    "            \n",
    "            \"AIzaSyB2KEWJrRO9hqHBQGYh5eMJR4-MQ4DB1Mo\",\n",
    "            \"AIzaSyAHeARtuLLIL-sInS07z_h7BwGlg9JrLWA\",\n",
    "            \"AIzaSyChagAJk_qPvVkz7DgtiHGallt-HoIOX2w\",\n",
    "            \"AIzaSyAF81-PjYUxVvkYBUPGHtl_eRQrsc-Muow\",\n",
    "            \"AIzaSyDW-AFnoOAAS6NR-0Ii8_LINfbs8iCb13U\",\n",
    "           \n",
    "            \"AIzaSyAOD_0FBQ9UIHb9SSJc276yUXjnvqR4U9Y\",\n",
    "            \"AIzaSyALhuxQd7E5whXErpSRzKiSo0AkUzHgZ0I\", \n",
    "            \"AIzaSyAUx3VdZFy9BL3zZ9IW7xesmm1Uk50B9m8\", \n",
    "            \"AIzaSyCQ1nsL1E7R3amaBf8nycoXIbazDfHc2h8\",\n",
    "\n",
    "            \"AIzaSyC-uc79BhqxpYv9lYn-gDv5Ksgsk6HAVkA\",\n",
    "            \"AIzaSyCpbaDn_1QrbpabT78XjYSDqqrBj9Mul3M\",\n",
    "            \"AIzaSyDZaHVcLMAt4ybeKmcg6kc1QWr7UImXUvk\",\n",
    "            \"AIzaSyBqPEXiaitV7JS4_bNur13Elds0QpcTkCw\",\n",
    "            \"AIzaSyAV-GK-LzA6tam7lHhLJ-_ysL4r4JKCWgA\",\n",
    "            \"AIzaSyAp-dIFALPjFBuknzbFQ5ijfDwy2idDxTc\",\n",
    "            \"AIzaSyDmHSXtCumihEQL2yr2qUxf3WQGzUA7Enc\",\n",
    "           \n",
    "            \"AIzaSyDFbglyzqzbrOdPsYXkL8X9rfFCdwiseH0\",\n",
    "            \"AIzaSyBwRfVuq1pDI7dhOgKVSBL6xX0U1NepQ8s\",\n",
    "            \"AIzaSyA8mbtCqaJLZn94cKlSM7IafHnWRh7JrNg\",\n",
    "            \"AIzaSyDxIIhH7h9tiS9Oe6lk2c1mZnCCVmaDviI\",\n",
    "            \"AIzaSyA1mGZ4N7RTABnIOEXdTdfGpz8xnHgwBNA\",\n",
    "\n",
    "            \"AIzaSyAdDUtZA1Jj_GIvX3EEos9j9eGSpXkODQs\",\n",
    "            \"AIzaSyAdfUdlPhINTAsf1CZ-NyBA2RGq3XIrZeg\",\n",
    "\n",
    "            \"AIzaSyC7Ohnil-cT9vKYY27hEF8xCJ4Yyov8nJI\",\n",
    "            \"AIzaSyA27JoITYWc2ooi3UGFlF-bmh2SIDh7Qv8\"\n",
    "           ]\n",
    "key_index = 0\n",
    "\n",
    "def get_youtube_service():\n",
    "    global key_index\n",
    "    key = API_KEYS[key_index % len(API_KEYS)]\n",
    "    key_index += 1\n",
    "    return build(\"youtube\", \"v3\", developerKey=key)\n",
    "\n",
    "youtube = get_youtube_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "761f8be0-4488-43a4-9f84-6c8f126c0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UTILS ---\n",
    "def safe_filename(name):\n",
    "    return re.sub(r\"[^\\w]+\", \"_\", name).strip(\"_\").lower()\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'[^a-z0-9@]', '', text.lower())\n",
    "\n",
    "def extract_mentions_tags(text):\n",
    "    mentions = re.findall(r'@([\\w\\d_]+)', text)\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    return [normalize_text(m) for m in mentions], hashtags\n",
    "\n",
    "def execute_with_backoff(request, max_retries=5):\n",
    "    for n in range(max_retries):\n",
    "        try:\n",
    "            return request.execute()\n",
    "        except HttpError as e:\n",
    "            if e.resp.status in [403, 500, 503]:\n",
    "                sleep_time = (2 ** n) + random.uniform(0, 1)\n",
    "                print(f\"[Backoff] Sleeping for {sleep_time:.2f}s due to error: {e}\")\n",
    "                time.sleep(sleep_time)\n",
    "                global youtube\n",
    "                youtube = get_youtube_service()\n",
    "            else:\n",
    "                raise\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52f9ff45-aef4-4c11-8793-fbf29d7841f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RESOLVE CHANNEL HANDLE ---\n",
    "def resolve_channel_handle(handle):\n",
    "    try:\n",
    "        response = execute_with_backoff(\n",
    "            youtube.search().list(q=f\"@{handle}\", type=\"channel\", part=\"snippet\", maxResults=1)\n",
    "        )\n",
    "        if response['items']:\n",
    "            return response['items'][0]['snippet']['channelId']\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to resolve @{handle}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c2f3571-c14c-4573-b586-7991dbbfc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GET VIDEO DETAILS ---\n",
    "def get_video_details(video_id):\n",
    "    try:\n",
    "        response = execute_with_backoff(\n",
    "            youtube.videos().list(part=\"snippet,statistics\", id=video_id)\n",
    "        )\n",
    "        if not response or not response['items']:\n",
    "            return None\n",
    "        item = response['items'][0]\n",
    "        snippet = item['snippet']\n",
    "        stats = item.get(\"statistics\", {})\n",
    "        desc = snippet.get(\"description\", \"\")\n",
    "        mentions, hashtags = extract_mentions_tags(desc)\n",
    "        return {\n",
    "            \"video_id\": video_id,\n",
    "            \"title\": snippet.get(\"title\"),\n",
    "            \"channel\": snippet.get(\"channelTitle\"),\n",
    "            \"channelId\": snippet.get(\"channelId\"),\n",
    "            \"published\": snippet.get(\"publishedAt\"),\n",
    "            \"description\": desc,\n",
    "            \"tags\": snippet.get(\"tags\", []),\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"viewCount\": int(stats.get(\"viewCount\", 0))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Video fetch failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7813ef5-aabc-4cf7-9ba3-6eba96f1ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GET COMMENTS & REPLIES ---\n",
    "def get_comments_and_replies(video_id, max_results=100):\n",
    "    comments, replies = [], []\n",
    "    try:\n",
    "        response = execute_with_backoff(\n",
    "            youtube.commentThreads().list(part=\"snippet,replies\", videoId=video_id, maxResults=max_results, textFormat=\"plainText\")\n",
    "        )\n",
    "        for item in response.get(\"items\", []):\n",
    "            top = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = item['snippet']['topLevelComment']['id']\n",
    "            comments.append({\n",
    "                \"comment_id\": comment_id,\n",
    "                \"video_id\": video_id,\n",
    "                \"author\": top['authorDisplayName'],\n",
    "                \"published_at\": top['publishedAt'],\n",
    "                \"text\": top['textDisplay']\n",
    "            })\n",
    "            for reply in item.get(\"replies\", {}).get(\"comments\", []):\n",
    "                replies.append({\n",
    "                    \"video_id\": video_id,\n",
    "                    \"in_reply_to\": comment_id,\n",
    "                    \"parent_author\": top['authorDisplayName'],\n",
    "                    \"author\": reply['snippet']['authorDisplayName'],\n",
    "                    \"published_at\": reply['snippet']['publishedAt'],\n",
    "                    \"text\": reply['snippet']['textDisplay']\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Comments fetch failed: {e}\")\n",
    "    return comments, replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "284c9b95-66b3-4eec-8559-8f9135b4916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN FUNCTION ---\n",
    "def search_and_save(query, max_pages=5):\n",
    "    query_safe = safe_filename(query)\n",
    "    os.makedirs(query_safe, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(os.path.join(query_safe, \"edges.csv\")):\n",
    "        print(f\"[SKIP] Already done: {query}\")\n",
    "        return\n",
    "\n",
    "    seen_video_ids = set()\n",
    "    edge_counter = defaultdict(lambda: {\"mentions\": 0, \"views\": 0, \"video_ids\": set()})\n",
    "    handle_to_channelId = {}\n",
    "\n",
    "    with open(os.path.join(query_safe, \"videos.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as vf, \\\n",
    "         open(os.path.join(query_safe, \"comments.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as cf, \\\n",
    "         open(os.path.join(query_safe, \"replies.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as rf:\n",
    "\n",
    "        vw = csv.DictWriter(vf, fieldnames=[\"video_id\", \"title\", \"channel\", \"channelId\", \"published\", \"description\", \"tags\", \"mentions\", \"hashtags\", \"views\"])\n",
    "        cw = csv.DictWriter(cf, fieldnames=[\"comment_id\", \"video_id\", \"author\", \"published_at\", \"text\"])\n",
    "        rw = csv.DictWriter(rf, fieldnames=[\"video_id\", \"in_reply_to\", \"parent_author\", \"author\", \"published_at\", \"text\"])\n",
    "        vw.writeheader(); cw.writeheader(); rw.writeheader()\n",
    "\n",
    "        next_page_token = None\n",
    "        for page in range(max_pages):\n",
    "            print(f\"[INFO] Query: {query} — Page {page + 1}/{max_pages}\")\n",
    "            response = execute_with_backoff(\n",
    "                youtube.search().list(q=query, type=\"video\", part=\"id\", maxResults=50, pageToken=next_page_token)\n",
    "            )\n",
    "            if not response: break\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                video_id = item['id']['videoId']\n",
    "                if video_id in seen_video_ids:\n",
    "                    continue\n",
    "                seen_video_ids.add(video_id)\n",
    "\n",
    "                video_data = get_video_details(video_id)\n",
    "                if not video_data:\n",
    "                    continue\n",
    "\n",
    "                vw.writerow({\n",
    "                    \"video_id\": video_id,\n",
    "                    **{k: video_data[k] for k in [\"title\", \"channel\", \"channelId\", \"published\", \"description\"]},\n",
    "                    \"tags\": \", \".join(video_data[\"tags\"]),\n",
    "                    \"mentions\": \", \".join(video_data[\"mentions\"]),\n",
    "                    \"hashtags\": \", \".join(video_data[\"hashtags\"]),\n",
    "                    \"views\": video_data[\"viewCount\"]\n",
    "                })\n",
    "\n",
    "                for mention in video_data[\"mentions\"]:\n",
    "                    if mention not in handle_to_channelId:\n",
    "                        channel_id = resolve_channel_handle(mention)\n",
    "                        if channel_id:\n",
    "                            handle_to_channelId[mention] = channel_id\n",
    "                        else:\n",
    "                            continue\n",
    "                    edge_key = (video_data[\"channelId\"], handle_to_channelId[mention])\n",
    "                    edge_counter[edge_key][\"mentions\"] += 1\n",
    "                    edge_counter[edge_key][\"views\"] += video_data[\"viewCount\"]\n",
    "                    edge_counter[edge_key][\"video_ids\"].add(video_id)\n",
    "\n",
    "                comments, replies = get_comments_and_replies(video_id)\n",
    "                for c in comments: cw.writerow(c)\n",
    "                for r in replies: rw.writerow(r)\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "    with open(os.path.join(query_safe, \"edges.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as ef:\n",
    "        ew = csv.DictWriter(ef, fieldnames=[\"source_channelId\", \"target_channelId\", \"mention_count\", \"total_views\", \"video_ids\"])\n",
    "        ew.writeheader()\n",
    "        for (src, tgt), stats in edge_counter.items():\n",
    "            ew.writerow({\n",
    "                \"source_channelId\": src,\n",
    "                \"target_channelId\": tgt,\n",
    "                \"mention_count\": stats[\"mentions\"],\n",
    "                \"total_views\": stats[\"views\"],\n",
    "                \"video_ids\": \";\".join(stats[\"video_ids\"])\n",
    "            })\n",
    "\n",
    "    print(f\"[✔] Done: {query} — Data saved in '{query_safe}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da4d50af-c94e-4f18-a3b2-cefb674c7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN FUNCTION ---\n",
    "def search_and_save(query, max_pages=5):\n",
    "    query_safe = safe_filename(query)\n",
    "    os.makedirs(query_safe, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(os.path.join(query_safe, \"edges.csv\")):\n",
    "        print(f\"[SKIP] Already done: {query}\")\n",
    "        return\n",
    "\n",
    "    seen_video_ids = set()\n",
    "    edge_counter = defaultdict(lambda: {\"mentions\": 0, \"views\": 0, \"video_ids\": set()})\n",
    "    handle_to_channelId = {}\n",
    "\n",
    "    def crawl_channel_mentions(channel_id, current_depth=1, max_depth=1):\n",
    "        if current_depth > max_depth:\n",
    "            return\n",
    "        try:\n",
    "            response = execute_with_backoff(\n",
    "                youtube.search().list(part=\"id\", type=\"video\", channelId=channel_id, maxResults=5)\n",
    "            )\n",
    "            for item in response.get(\"items\", []):\n",
    "                vid = item[\"id\"][\"videoId\"]\n",
    "                vdata = get_video_details(vid)\n",
    "                if not vdata:\n",
    "                    continue\n",
    "                for m in vdata[\"mentions\"]:\n",
    "                    if m not in handle_to_channelId:\n",
    "                        resolved = resolve_channel_handle(m)\n",
    "                        if resolved:\n",
    "                            handle_to_channelId[m] = resolved\n",
    "                        else:\n",
    "                            continue\n",
    "                    edge_key = (channel_id, handle_to_channelId[m])\n",
    "                    edge_counter[edge_key][\"mentions\"] += 1\n",
    "                    edge_counter[edge_key][\"video_ids\"].add(vid)\n",
    "        except Exception as e:\n",
    "            print(f\"[Recurse WARN] {e}\")\n",
    "\n",
    "    with open(os.path.join(query_safe, \"videos.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as vf,          open(os.path.join(query_safe, \"comments.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as cf,          open(os.path.join(query_safe, \"replies.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as rf:\n",
    "\n",
    "        vw = csv.DictWriter(vf, fieldnames=[\"video_id\", \"title\", \"channel\", \"channelId\", \"published\", \"description\", \"tags\", \"mentions\", \"hashtags\", \"views\"])\n",
    "        cw = csv.DictWriter(cf, fieldnames=[\"comment_id\", \"video_id\", \"author\", \"published_at\", \"text\"])\n",
    "        rw = csv.DictWriter(rf, fieldnames=[\"video_id\", \"in_reply_to\", \"parent_author\", \"author\", \"published_at\", \"text\"])\n",
    "        vw.writeheader(); cw.writeheader(); rw.writeheader()\n",
    "\n",
    "        next_page_token = None\n",
    "        for page in range(max_pages):\n",
    "            print(f\"[INFO] Query: {query} — Page {page + 1}/{max_pages}\")\n",
    "            response = execute_with_backoff(\n",
    "                youtube.search().list(q=query, type=\"video\", part=\"id\", maxResults=50, pageToken=next_page_token)\n",
    "            )\n",
    "            if not response: break\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                video_id = item['id']['videoId']\n",
    "                if video_id in seen_video_ids:\n",
    "                    continue\n",
    "                seen_video_ids.add(video_id)\n",
    "\n",
    "                video_data = get_video_details(video_id)\n",
    "                if not video_data:\n",
    "                    continue\n",
    "\n",
    "                vw.writerow({\n",
    "                    \"video_id\": video_id,\n",
    "                    **{k: video_data[k] for k in [\"title\", \"channel\", \"channelId\", \"published\", \"description\"]},\n",
    "                    \"tags\": \", \".join(video_data[\"tags\"]),\n",
    "                    \"mentions\": \", \".join(video_data[\"mentions\"]),\n",
    "                    \"hashtags\": \", \".join(video_data[\"hashtags\"]),\n",
    "                    \"views\": video_data[\"viewCount\"]\n",
    "                })\n",
    "\n",
    "                # Layer 1\n",
    "                for mention in video_data[\"mentions\"]:\n",
    "                    if mention not in handle_to_channelId:\n",
    "                        channel_id = resolve_channel_handle(mention)\n",
    "                        if channel_id:\n",
    "                            handle_to_channelId[mention] = channel_id\n",
    "                        else:\n",
    "                            continue\n",
    "                    edge_key = (video_data[\"channelId\"], handle_to_channelId[mention])\n",
    "                    edge_counter[edge_key][\"mentions\"] += 1\n",
    "                    edge_counter[edge_key][\"views\"] += video_data[\"viewCount\"]\n",
    "                    edge_counter[edge_key][\"video_ids\"].add(video_id)\n",
    "\n",
    "                # Layer 4: Recursively analyze mentioned channels\n",
    "                for m in video_data[\"mentions\"]:\n",
    "                    ch_id = handle_to_channelId.get(m)\n",
    "                    if ch_id:\n",
    "                        crawl_channel_mentions(ch_id)\n",
    "\n",
    "                comments, replies = get_comments_and_replies(video_id)\n",
    "                for c in comments: cw.writerow(c)\n",
    "                for r in replies: rw.writerow(r)\n",
    "\n",
    "                # Layer 2: Commenter → Mentioned Handle\n",
    "                for comment in comments:\n",
    "                    c_mentions, _ = extract_mentions_tags(comment[\"text\"])\n",
    "                    for m in c_mentions:\n",
    "                        if m not in handle_to_channelId:\n",
    "                            resolved = resolve_channel_handle(m)\n",
    "                            if resolved:\n",
    "                                handle_to_channelId[m] = resolved\n",
    "                            else:\n",
    "                                continue\n",
    "                        edge_key = (comment[\"author\"], handle_to_channelId[m])\n",
    "                        edge_counter[edge_key][\"mentions\"] += 1\n",
    "                        edge_counter[edge_key][\"video_ids\"].add(video_id)\n",
    "\n",
    "                # Layer 3: Replier → Parent Commenter\n",
    "                for reply in replies:\n",
    "                    if reply[\"author\"] != reply[\"parent_author\"]:\n",
    "                        edge_key = (reply[\"author\"], reply[\"parent_author\"])\n",
    "                        edge_counter[edge_key][\"mentions\"] += 1\n",
    "                        edge_counter[edge_key][\"video_ids\"].add(reply[\"video_id\"])\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "    with open(os.path.join(query_safe, \"edges.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as ef:\n",
    "        ew = csv.DictWriter(ef, fieldnames=[\"source_channelId\", \"target_channelId\", \"mention_count\", \"total_views\", \"video_ids\"])\n",
    "        ew.writeheader()\n",
    "        for (src, tgt), stats in edge_counter.items():\n",
    "            ew.writerow({\n",
    "                \"source_channelId\": src,\n",
    "                \"target_channelId\": tgt,\n",
    "                \"mention_count\": stats[\"mentions\"],\n",
    "                \"total_views\": stats.get(\"views\", 0),\n",
    "                \"video_ids\": \";\".join(stats[\"video_ids\"])\n",
    "            })\n",
    "\n",
    "    print(f\"[✔] Done: {query} — Data saved in '{query_safe}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b852f151-32f8-4ace-8e45-f9b94b309267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Query: sabrina carpenter vlog — Page 1/5\n",
      "[Backoff] Sleeping for 1.07s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=sabrina+carpenter+vlog&type=video&part=id&maxResults=50&key=AIzaSyDPSqJudfBLzj6Z-izKUN6DmdaSPC8WFpg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 2.43s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=sabrina+carpenter+vlog&type=video&part=id&maxResults=50&key=AIzaSyDPSqJudfBLzj6Z-izKUN6DmdaSPC8WFpg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 4.79s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=sabrina+carpenter+vlog&type=video&part=id&maxResults=50&key=AIzaSyDPSqJudfBLzj6Z-izKUN6DmdaSPC8WFpg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 8.23s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=sabrina+carpenter+vlog&type=video&part=id&maxResults=50&key=AIzaSyDPSqJudfBLzj6Z-izKUN6DmdaSPC8WFpg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 16.65s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=sabrina+carpenter+vlog&type=video&part=id&maxResults=50&key=AIzaSyDPSqJudfBLzj6Z-izKUN6DmdaSPC8WFpg&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Max retries exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m      2\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# \"Sabrina Carpenter music video\", \u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# \"Sabrina Carpenter personal life\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msabrina carpenter vlog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m---> 16\u001b[0m     \u001b[43msearch_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 49\u001b[0m, in \u001b[0;36msearch_and_save\u001b[1;34m(query, max_pages)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_pages):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m — Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_pages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_with_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43myoutube\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxResults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpageToken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_page_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "Cell \u001b[1;32mIn[20], line 26\u001b[0m, in \u001b[0;36mexecute_with_backoff\u001b[1;34m(request, max_retries)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Max retries exceeded"
     ]
    }
   ],
   "source": [
    "# --- QUERIES ---\n",
    "queries = [\n",
    "    # \"Sabrina Carpenter music video\", \n",
    "    # \"Sabrina Carpenter personal life\",\n",
    "    # \"Sabrina Carpenter reactions\",\n",
    "    # \"Sabrina Carpenter interviews\",\n",
    "    \n",
    "    # \"Sabrina Carpenter acting career\",\n",
    "    # \"Sabrina Carpenter live performance\",\n",
    "    # \" Sabrina Carpenter controversy\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    search_and_save(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d682a2-c44a-404a-b55f-21c48d377f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0cfbf-db5c-4e1e-8a18-1a63e60e41e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea5672-36c1-4f2d-b6b8-3bbfcf5fb81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940e134-4674-48ef-8f48-5263357cc720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7531511-0989-43aa-b5f2-6b9b85a538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "DAILY_QUOTA_PER_KEY = 9500\n",
    "QUOTA_UNIT_PER_REQUEST = 1\n",
    "MAX_DAILY_QUOTA = DAILY_QUOTA_PER_KEY * len(API_KEYS)\n",
    "\n",
    "# === YOUTUBE SERVICE ROTATOR ===\n",
    "api_key_index = 0\n",
    "\n",
    "def get_youtube_service():\n",
    "    global api_key_index\n",
    "    api_key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "def execute_with_backoff(request):\n",
    "    retries = 5\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            return request.execute()\n",
    "        except HttpError as e:\n",
    "            if e.resp.status in [403, 500, 503]:\n",
    "                wait = (2 ** i) + random.uniform(0, 1)\n",
    "                print(f\"Retrying in {wait:.2f} seconds due to error: {e}\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c71434e-9664-4790-9109-2f2b8bba126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_channel_names(channel_ids, cache_file=\"channel_names.csv\"):\n",
    "    import os\n",
    "\n",
    "    # Load existing cache\n",
    "    if os.path.exists(cache_file):\n",
    "        existing_df = pd.read_csv(cache_file)\n",
    "        existing = dict(zip(existing_df[\"channel_id\"], existing_df[\"channel_name\"]))\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\"channel_id\", \"channel_name\"])\n",
    "        existing = {}\n",
    "\n",
    "    # Filter out only valid channel IDs (i.e., those starting with 'UC')\n",
    "    valid_channel_ids = [cid for cid in channel_ids if isinstance(cid, str) and cid.startswith(\"UC\")]\n",
    "    to_fetch = list(set(valid_channel_ids) - set(existing.keys()))\n",
    "\n",
    "    print(f\"Found {len(existing)} already fetched. Fetching {len(to_fetch)} new valid channel IDs...\")\n",
    "\n",
    "    new_data = []\n",
    "    quota_used = 0\n",
    "\n",
    "    for i in range(0, len(to_fetch), 50):\n",
    "        if quota_used >= MAX_DAILY_QUOTA:\n",
    "            print(\"Reached daily quota limit.\")\n",
    "            break\n",
    "\n",
    "        batch = to_fetch[i:i + 50]\n",
    "        youtube = get_youtube_service()\n",
    "\n",
    "        try:\n",
    "            response = execute_with_backoff(\n",
    "                youtube.channels().list(part=\"snippet\", id=\",\".join(batch))\n",
    "            )\n",
    "            for item in response.get(\"items\", []):\n",
    "                cid = item[\"id\"]\n",
    "                title = item[\"snippet\"][\"title\"]\n",
    "                existing[cid] = title\n",
    "                new_data.append((cid, title))\n",
    "\n",
    "        except HttpError as e:\n",
    "            error_reason = None\n",
    "            try:\n",
    "                error_reason = e.error_details[0]['reason']\n",
    "            except:\n",
    "                pass\n",
    "            print(f\"Error: {e}\")\n",
    "            if \"quotaExceeded\" in str(e) or error_reason == \"quotaExceeded\":\n",
    "                print(\"Quota exceeded. Stopping fetch.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Write batch immediately to cache to avoid loss\n",
    "        if new_data:\n",
    "            batch_df = pd.DataFrame(new_data, columns=[\"channel_id\", \"channel_name\"])\n",
    "            existing_df = pd.concat([existing_df, batch_df], ignore_index=True).drop_duplicates(subset=\"channel_id\")\n",
    "            existing_df.to_csv(cache_file, index=False)\n",
    "            print(f\"Saved batch of {len(new_data)}. Total cached: {len(existing_df)}\")\n",
    "            new_data = []\n",
    "\n",
    "        quota_used += len(batch)\n",
    "        time.sleep(0.1)  # Optional to reduce rate\n",
    "\n",
    "    print(\"Channel name fetching completed.\")\n",
    "    return existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a6c91fb-d191-488c-b58c-31f72f338fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_channel_names_to_csv(input_file, output_file, columns_to_map, name_map_df):\n",
    "    df = pd.read_csv(input_file)\n",
    "    map_dict = dict(zip(name_map_df[\"channel_id\"], name_map_df[\"channel_name\"]))\n",
    "\n",
    "    for col in columns_to_map:\n",
    "        name_col = f\"{col}_name\"\n",
    "        df[name_col] = df[col].map(map_dict).fillna(\"Unknown\")\n",
    "\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved updated file: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1a63a0fd-e898-4acb-bb5c-da466bd28a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 already fetched. Fetching 61 new valid channel IDs...\n",
      "[Backoff] Sleeping for 1.20s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/channels?part=snippet&id=UCVjj_osiKAP_qTtJAr6IrvA%2CUCJJhdAgEbBGZHUa5n0tBP3g%2CUCT7xAt9yyxfBnks4k9XyAkQ%2CUC0LfERYIbu4Qc0ssLiDWdNw%2CUCzBm4uZ0RKT1u8CSdSwzG-w%2CUCIPM9B55iGbfpbxQh4Sya-g%2CUCe7KQjGEUloxCv5yB4fSIOA%2CUCWljxewHlJE3M7U_6_zFNyA%2CUCVVuSskTN_iHl_A-Tb8yA1w%2CUC_6hQy4elsyHhCOskZo0U5g%2CUCfXFq0qbfJj5q35l6HE-7sg%2CUCR1D15p_vdP3HkrH8wgjQRw%2CUCE-nOavlAVYoXGKS6zxvodA%2CUCPKWE1H6xhxwPlqUlKgHb_w%2CUCICMWH9LAtAh8sd9FoOfEfQ%2CUCYRE3PMYatMVXmqmulF0sgA%2CUCNGIytL4kzna9zJzAHaoZ2g%2CUCyYhxPxa-bYr-IIAGIV33PQ%2CUCO11MeHgFejntXiaqFLyo_w%2CUChfrWzVNU6qN0690-7yLoiw%2CUCpbWpuupuBZS0wtvLPOcuWQ%2CUCjYlRtRXsSQN_wYokukB-Ew%2CUCaaItXO5SYv_MTuExUlh-tA%2CUCQLvcN41js-UutOFkUFHLmA%2CUC39eHhTEqiBlqRbmIIYjm-g%2CUC_cvTMeip9po2hZdF3aBXrA%2CUCEMVnIDQua4ZJlUsQ6G775w%2CUC4oVjJptcsvtgIMrdIUQYlQ%2CUC-6-LnSINysVagonyKlQBcg%2CUCflpy-GNxlahbdRdN5CRUsA%2CUCaHT88aobpcvRFEuy4v5Clg%2CUCfuXHyUwTXDTpy-gnyM1LhA%2CUC4mYKePIas5YgGfYZ2J2OxQ%2CUCIBbuLxWNh8pN3tuDYXX7zA%2CUCoNrnH-JR4O6C_0T6iUiEvg%2CUCbnagt1TEDo9U0yihgKNJ0g%2CUCe976mp6mG3Br6rrjx2CzMQ%2CUCyWRnDFP3BEEED1mB_Zug2w%2CUCV0IJI5nnLtheMSyG91KzLw%2CUC4KjMNOZe0mzayYzTJ5WckA%2CUCjgCdcb-z8MNZInajCu84sg%2CUCEI4SXIzu5Oxusjpn6HFCvw%2CUCa961mgEIqtMaShHv2zzaHA%2CUCYj-yyXdV_1CtNFTjusiWIg%2CUCNRkbeiJOcwqmBjgl3xnW1w%2CUCp1AeV2SoE-6HFkjU-9T5Vw%2CUCy_Aj0ofokSBI1SaLk_hPCw%2CUCqfqdCqPHxgtdJ0m32obDiw%2CUCy44aw2uaCYF4lqEq-W0qgg%2CUCUa1cHle6CSjw_I1HTtsqbg&key=AIzaSyA27JoITYWc2ooi3UGFlF-bmh2SIDh7Qv8&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 2.95s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/channels?part=snippet&id=UCVjj_osiKAP_qTtJAr6IrvA%2CUCJJhdAgEbBGZHUa5n0tBP3g%2CUCT7xAt9yyxfBnks4k9XyAkQ%2CUC0LfERYIbu4Qc0ssLiDWdNw%2CUCzBm4uZ0RKT1u8CSdSwzG-w%2CUCIPM9B55iGbfpbxQh4Sya-g%2CUCe7KQjGEUloxCv5yB4fSIOA%2CUCWljxewHlJE3M7U_6_zFNyA%2CUCVVuSskTN_iHl_A-Tb8yA1w%2CUC_6hQy4elsyHhCOskZo0U5g%2CUCfXFq0qbfJj5q35l6HE-7sg%2CUCR1D15p_vdP3HkrH8wgjQRw%2CUCE-nOavlAVYoXGKS6zxvodA%2CUCPKWE1H6xhxwPlqUlKgHb_w%2CUCICMWH9LAtAh8sd9FoOfEfQ%2CUCYRE3PMYatMVXmqmulF0sgA%2CUCNGIytL4kzna9zJzAHaoZ2g%2CUCyYhxPxa-bYr-IIAGIV33PQ%2CUCO11MeHgFejntXiaqFLyo_w%2CUChfrWzVNU6qN0690-7yLoiw%2CUCpbWpuupuBZS0wtvLPOcuWQ%2CUCjYlRtRXsSQN_wYokukB-Ew%2CUCaaItXO5SYv_MTuExUlh-tA%2CUCQLvcN41js-UutOFkUFHLmA%2CUC39eHhTEqiBlqRbmIIYjm-g%2CUC_cvTMeip9po2hZdF3aBXrA%2CUCEMVnIDQua4ZJlUsQ6G775w%2CUC4oVjJptcsvtgIMrdIUQYlQ%2CUC-6-LnSINysVagonyKlQBcg%2CUCflpy-GNxlahbdRdN5CRUsA%2CUCaHT88aobpcvRFEuy4v5Clg%2CUCfuXHyUwTXDTpy-gnyM1LhA%2CUC4mYKePIas5YgGfYZ2J2OxQ%2CUCIBbuLxWNh8pN3tuDYXX7zA%2CUCoNrnH-JR4O6C_0T6iUiEvg%2CUCbnagt1TEDo9U0yihgKNJ0g%2CUCe976mp6mG3Br6rrjx2CzMQ%2CUCyWRnDFP3BEEED1mB_Zug2w%2CUCV0IJI5nnLtheMSyG91KzLw%2CUC4KjMNOZe0mzayYzTJ5WckA%2CUCjgCdcb-z8MNZInajCu84sg%2CUCEI4SXIzu5Oxusjpn6HFCvw%2CUCa961mgEIqtMaShHv2zzaHA%2CUCYj-yyXdV_1CtNFTjusiWIg%2CUCNRkbeiJOcwqmBjgl3xnW1w%2CUCp1AeV2SoE-6HFkjU-9T5Vw%2CUCy_Aj0ofokSBI1SaLk_hPCw%2CUCqfqdCqPHxgtdJ0m32obDiw%2CUCy44aw2uaCYF4lqEq-W0qgg%2CUCUa1cHle6CSjw_I1HTtsqbg&key=AIzaSyA27JoITYWc2ooi3UGFlF-bmh2SIDh7Qv8&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 4.08s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/channels?part=snippet&id=UCVjj_osiKAP_qTtJAr6IrvA%2CUCJJhdAgEbBGZHUa5n0tBP3g%2CUCT7xAt9yyxfBnks4k9XyAkQ%2CUC0LfERYIbu4Qc0ssLiDWdNw%2CUCzBm4uZ0RKT1u8CSdSwzG-w%2CUCIPM9B55iGbfpbxQh4Sya-g%2CUCe7KQjGEUloxCv5yB4fSIOA%2CUCWljxewHlJE3M7U_6_zFNyA%2CUCVVuSskTN_iHl_A-Tb8yA1w%2CUC_6hQy4elsyHhCOskZo0U5g%2CUCfXFq0qbfJj5q35l6HE-7sg%2CUCR1D15p_vdP3HkrH8wgjQRw%2CUCE-nOavlAVYoXGKS6zxvodA%2CUCPKWE1H6xhxwPlqUlKgHb_w%2CUCICMWH9LAtAh8sd9FoOfEfQ%2CUCYRE3PMYatMVXmqmulF0sgA%2CUCNGIytL4kzna9zJzAHaoZ2g%2CUCyYhxPxa-bYr-IIAGIV33PQ%2CUCO11MeHgFejntXiaqFLyo_w%2CUChfrWzVNU6qN0690-7yLoiw%2CUCpbWpuupuBZS0wtvLPOcuWQ%2CUCjYlRtRXsSQN_wYokukB-Ew%2CUCaaItXO5SYv_MTuExUlh-tA%2CUCQLvcN41js-UutOFkUFHLmA%2CUC39eHhTEqiBlqRbmIIYjm-g%2CUC_cvTMeip9po2hZdF3aBXrA%2CUCEMVnIDQua4ZJlUsQ6G775w%2CUC4oVjJptcsvtgIMrdIUQYlQ%2CUC-6-LnSINysVagonyKlQBcg%2CUCflpy-GNxlahbdRdN5CRUsA%2CUCaHT88aobpcvRFEuy4v5Clg%2CUCfuXHyUwTXDTpy-gnyM1LhA%2CUC4mYKePIas5YgGfYZ2J2OxQ%2CUCIBbuLxWNh8pN3tuDYXX7zA%2CUCoNrnH-JR4O6C_0T6iUiEvg%2CUCbnagt1TEDo9U0yihgKNJ0g%2CUCe976mp6mG3Br6rrjx2CzMQ%2CUCyWRnDFP3BEEED1mB_Zug2w%2CUCV0IJI5nnLtheMSyG91KzLw%2CUC4KjMNOZe0mzayYzTJ5WckA%2CUCjgCdcb-z8MNZInajCu84sg%2CUCEI4SXIzu5Oxusjpn6HFCvw%2CUCa961mgEIqtMaShHv2zzaHA%2CUCYj-yyXdV_1CtNFTjusiWIg%2CUCNRkbeiJOcwqmBjgl3xnW1w%2CUCp1AeV2SoE-6HFkjU-9T5Vw%2CUCy_Aj0ofokSBI1SaLk_hPCw%2CUCqfqdCqPHxgtdJ0m32obDiw%2CUCy44aw2uaCYF4lqEq-W0qgg%2CUCUa1cHle6CSjw_I1HTtsqbg&key=AIzaSyA27JoITYWc2ooi3UGFlF-bmh2SIDh7Qv8&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 8.69s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/channels?part=snippet&id=UCVjj_osiKAP_qTtJAr6IrvA%2CUCJJhdAgEbBGZHUa5n0tBP3g%2CUCT7xAt9yyxfBnks4k9XyAkQ%2CUC0LfERYIbu4Qc0ssLiDWdNw%2CUCzBm4uZ0RKT1u8CSdSwzG-w%2CUCIPM9B55iGbfpbxQh4Sya-g%2CUCe7KQjGEUloxCv5yB4fSIOA%2CUCWljxewHlJE3M7U_6_zFNyA%2CUCVVuSskTN_iHl_A-Tb8yA1w%2CUC_6hQy4elsyHhCOskZo0U5g%2CUCfXFq0qbfJj5q35l6HE-7sg%2CUCR1D15p_vdP3HkrH8wgjQRw%2CUCE-nOavlAVYoXGKS6zxvodA%2CUCPKWE1H6xhxwPlqUlKgHb_w%2CUCICMWH9LAtAh8sd9FoOfEfQ%2CUCYRE3PMYatMVXmqmulF0sgA%2CUCNGIytL4kzna9zJzAHaoZ2g%2CUCyYhxPxa-bYr-IIAGIV33PQ%2CUCO11MeHgFejntXiaqFLyo_w%2CUChfrWzVNU6qN0690-7yLoiw%2CUCpbWpuupuBZS0wtvLPOcuWQ%2CUCjYlRtRXsSQN_wYokukB-Ew%2CUCaaItXO5SYv_MTuExUlh-tA%2CUCQLvcN41js-UutOFkUFHLmA%2CUC39eHhTEqiBlqRbmIIYjm-g%2CUC_cvTMeip9po2hZdF3aBXrA%2CUCEMVnIDQua4ZJlUsQ6G775w%2CUC4oVjJptcsvtgIMrdIUQYlQ%2CUC-6-LnSINysVagonyKlQBcg%2CUCflpy-GNxlahbdRdN5CRUsA%2CUCaHT88aobpcvRFEuy4v5Clg%2CUCfuXHyUwTXDTpy-gnyM1LhA%2CUC4mYKePIas5YgGfYZ2J2OxQ%2CUCIBbuLxWNh8pN3tuDYXX7zA%2CUCoNrnH-JR4O6C_0T6iUiEvg%2CUCbnagt1TEDo9U0yihgKNJ0g%2CUCe976mp6mG3Br6rrjx2CzMQ%2CUCyWRnDFP3BEEED1mB_Zug2w%2CUCV0IJI5nnLtheMSyG91KzLw%2CUC4KjMNOZe0mzayYzTJ5WckA%2CUCjgCdcb-z8MNZInajCu84sg%2CUCEI4SXIzu5Oxusjpn6HFCvw%2CUCa961mgEIqtMaShHv2zzaHA%2CUCYj-yyXdV_1CtNFTjusiWIg%2CUCNRkbeiJOcwqmBjgl3xnW1w%2CUCp1AeV2SoE-6HFkjU-9T5Vw%2CUCy_Aj0ofokSBI1SaLk_hPCw%2CUCqfqdCqPHxgtdJ0m32obDiw%2CUCy44aw2uaCYF4lqEq-W0qgg%2CUCUa1cHle6CSjw_I1HTtsqbg&key=AIzaSyA27JoITYWc2ooi3UGFlF-bmh2SIDh7Qv8&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "[Backoff] Sleeping for 16.92s due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/channels?part=snippet&id=UCVjj_osiKAP_qTtJAr6IrvA%2CUCJJhdAgEbBGZHUa5n0tBP3g%2CUCT7xAt9yyxfBnks4k9XyAkQ%2CUC0LfERYIbu4Qc0ssLiDWdNw%2CUCzBm4uZ0RKT1u8CSdSwzG-w%2CUCIPM9B55iGbfpbxQh4Sya-g%2CUCe7KQjGEUloxCv5yB4fSIOA%2CUCWljxewHlJE3M7U_6_zFNyA%2CUCVVuSskTN_iHl_A-Tb8yA1w%2CUC_6hQy4elsyHhCOskZo0U5g%2CUCfXFq0qbfJj5q35l6HE-7sg%2CUCR1D15p_vdP3HkrH8wgjQRw%2CUCE-nOavlAVYoXGKS6zxvodA%2CUCPKWE1H6xhxwPlqUlKgHb_w%2CUCICMWH9LAtAh8sd9FoOfEfQ%2CUCYRE3PMYatMVXmqmulF0sgA%2CUCNGIytL4kzna9zJzAHaoZ2g%2CUCyYhxPxa-bYr-IIAGIV33PQ%2CUCO11MeHgFejntXiaqFLyo_w%2CUChfrWzVNU6qN0690-7yLoiw%2CUCpbWpuupuBZS0wtvLPOcuWQ%2CUCjYlRtRXsSQN_wYokukB-Ew%2CUCaaItXO5SYv_MTuExUlh-tA%2CUCQLvcN41js-UutOFkUFHLmA%2CUC39eHhTEqiBlqRbmIIYjm-g%2CUC_cvTMeip9po2hZdF3aBXrA%2CUCEMVnIDQua4ZJlUsQ6G775w%2CUC4oVjJptcsvtgIMrdIUQYlQ%2CUC-6-LnSINysVagonyKlQBcg%2CUCflpy-GNxlahbdRdN5CRUsA%2CUCaHT88aobpcvRFEuy4v5Clg%2CUCfuXHyUwTXDTpy-gnyM1LhA%2CUC4mYKePIas5YgGfYZ2J2OxQ%2CUCIBbuLxWNh8pN3tuDYXX7zA%2CUCoNrnH-JR4O6C_0T6iUiEvg%2CUCbnagt1TEDo9U0yihgKNJ0g%2CUCe976mp6mG3Br6rrjx2CzMQ%2CUCyWRnDFP3BEEED1mB_Zug2w%2CUCV0IJI5nnLtheMSyG91KzLw%2CUC4KjMNOZe0mzayYzTJ5WckA%2CUCjgCdcb-z8MNZInajCu84sg%2CUCEI4SXIzu5Oxusjpn6HFCvw%2CUCa961mgEIqtMaShHv2zzaHA%2CUCYj-yyXdV_1CtNFTjusiWIg%2CUCNRkbeiJOcwqmBjgl3xnW1w%2CUCp1AeV2SoE-6HFkjU-9T5Vw%2CUCy_Aj0ofokSBI1SaLk_hPCw%2CUCqfqdCqPHxgtdJ0m32obDiw%2CUCy44aw2uaCYF4lqEq-W0qgg%2CUCUa1cHle6CSjw_I1HTtsqbg&key=AIzaSyA27JoITYWc2ooi3UGFlF-bmh2SIDh7Qv8&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Max retries exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 25\u001b[0m\n\u001b[0;32m     17\u001b[0m     add_channel_names_to_csv(\n\u001b[0;32m     18\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     19\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges_named.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     20\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_channelId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_channelId\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     21\u001b[0m         name_df,\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[105], line 14\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Fetch and cache channel names\u001b[39;00m\n\u001b[0;32m     13\u001b[0m cache_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_names.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m name_df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_channel_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Add names to each file\u001b[39;00m\n\u001b[0;32m     17\u001b[0m add_channel_names_to_csv(\n\u001b[0;32m     18\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges_named.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     20\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_channelId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_channelId\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     21\u001b[0m     name_df,\n\u001b[0;32m     22\u001b[0m )\n",
      "Cell \u001b[1;32mIn[85], line 30\u001b[0m, in \u001b[0;36mfetch_channel_names\u001b[1;34m(channel_ids, cache_file)\u001b[0m\n\u001b[0;32m     27\u001b[0m youtube \u001b[38;5;241m=\u001b[39m get_youtube_service()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_with_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43myoutube\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnippet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n\u001b[0;32m     34\u001b[0m         cid \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[96], line 26\u001b[0m, in \u001b[0;36mexecute_with_backoff\u001b[1;34m(request, max_retries)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Max retries exceeded"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    base_path = \"sabrina_carpenter_controversy\"\n",
    "    # \"sabrina_carpenter_vlog\"\n",
    "\n",
    "    # Load all channel/account IDs from CSVs\n",
    "    edge_df = pd.read_csv(os.path.join(base_path, \"edges.csv\"))\n",
    "\n",
    "    ids = set()\n",
    "    ids.update(edge_df[\"source_channelId\"].dropna().unique())\n",
    "    ids.update(edge_df[\"target_channelId\"].dropna().unique())\n",
    "    \n",
    "    # Fetch and cache channel names\n",
    "    cache_file = os.path.join(base_path, \"channel_names.csv\")\n",
    "    name_df = fetch_channel_names(list(ids), cache_file=cache_file)\n",
    "\n",
    "    # Add names to each file\n",
    "    add_channel_names_to_csv(\n",
    "        os.path.join(base_path, \"edges.csv\"),\n",
    "        os.path.join(base_path, \"edges_named.csv\"),\n",
    "        [\"source_channelId\", \"target_channelId\"],\n",
    "        name_df,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa902857-a37e-4b81-bb8f-0b708fe3dce4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a158751b-b09d-4224-8ecc-5bf50c76eb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sabrina_carpenter_personal_life\\comments.csv...\n",
      "Loading sabrina_carpenter_music_video\\comments.csv...\n",
      "Loading sabrina_carpenter_reactions\\comments.csv...\n",
      "Loading sabrina_carpenter_interviews\\comments.csv...\n",
      "Loading sabrina_carpenter_acting_career\\comments.csv...\n",
      "Loading sabrina_carpenter_live_performance\\comments.csv...\n",
      "Loading sabrina_carpenter_controversy\\comments.csv...\n",
      "✅ Combined file saved as: comments_combined.csv\n",
      "📊 Columns in comments_combined.csv:\n",
      "['comment_id', 'video_id', 'author', 'published_at', 'text', 'source_folder']\n",
      "------------------------------------------------------------\n",
      "Loading sabrina_carpenter_personal_life\\replies.csv...\n",
      "Loading sabrina_carpenter_music_video\\replies.csv...\n",
      "Loading sabrina_carpenter_reactions\\replies.csv...\n",
      "Loading sabrina_carpenter_interviews\\replies.csv...\n",
      "Loading sabrina_carpenter_acting_career\\replies.csv...\n",
      "Loading sabrina_carpenter_live_performance\\replies.csv...\n",
      "Loading sabrina_carpenter_controversy\\replies.csv...\n",
      "✅ Combined file saved as: replies_combined.csv\n",
      "📊 Columns in replies_combined.csv:\n",
      "['video_id', 'in_reply_to', 'parent_author', 'author', 'published_at', 'text', 'source_folder']\n",
      "------------------------------------------------------------\n",
      "Loading sabrina_carpenter_personal_life\\videos.csv...\n",
      "Loading sabrina_carpenter_music_video\\videos.csv...\n",
      "Loading sabrina_carpenter_reactions\\videos.csv...\n",
      "Loading sabrina_carpenter_interviews\\videos.csv...\n",
      "Loading sabrina_carpenter_acting_career\\videos.csv...\n",
      "Loading sabrina_carpenter_live_performance\\videos.csv...\n",
      "Loading sabrina_carpenter_controversy\\videos.csv...\n",
      "✅ Combined file saved as: videos_combined.csv\n",
      "📊 Columns in videos_combined.csv:\n",
      "['video_id', 'title', 'channel', 'channelId', 'published', 'description', 'tags', 'mentions', 'hashtags', 'views', 'source_folder']\n",
      "------------------------------------------------------------\n",
      "Loading sabrina_carpenter_personal_life\\edges_named.csv...\n",
      "Loading sabrina_carpenter_music_video\\edges_named.csv...\n",
      "File edges_named.csv not found in sabrina_carpenter_reactions\n",
      "File edges_named.csv not found in sabrina_carpenter_interviews\n",
      "File edges_named.csv not found in sabrina_carpenter_acting_career\n",
      "File edges_named.csv not found in sabrina_carpenter_live_performance\n",
      "File edges_named.csv not found in sabrina_carpenter_controversy\n",
      "✅ Combined file saved as: edges_combined.csv\n",
      "📊 Columns in edges_combined.csv:\n",
      "['source_channelId', 'target_channelId', 'mention_count', 'total_views', 'video_ids', 'source_channelId_name', 'target_channelId_name', 'source_folder']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List of all relevant folders\n",
    "folders = [\n",
    "    \"sabrina_carpenter_personal_life\",\n",
    "    \"sabrina_carpenter_music_video\",\n",
    "    \"sabrina_carpenter_reactions\",\n",
    "    \"sabrina_carpenter_interviews\",\n",
    "    \"sabrina_carpenter_acting_career\",\n",
    "    \"sabrina_carpenter_live_performance\",\n",
    "    \"sabrina_carpenter_controversy\"\n",
    "]\n",
    "\n",
    "# Map of input filenames (to look for in each folder) and the corresponding combined output filename\n",
    "file_map = {\n",
    "    \"comments.csv\": \"comments_combined.csv\",\n",
    "    \"replies.csv\": \"replies_combined.csv\",\n",
    "    \"videos.csv\": \"videos_combined.csv\",\n",
    "    \"edges_named.csv\": \"edges_combined.csv\"\n",
    "}\n",
    "\n",
    "def combine_files():\n",
    "    for input_file, output_file in file_map.items():\n",
    "        combined_df = pd.DataFrame()\n",
    "        for folder in folders:\n",
    "            file_path = os.path.join(folder, input_file)\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"Loading {file_path}...\")\n",
    "                df = pd.read_csv(file_path)\n",
    "                df[\"source_folder\"] = folder  # Optional: track origin\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"File {input_file} not found in {folder}\")\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined file saved as: {output_file}\")\n",
    "        print(f\"Columns in {output_file}:\")\n",
    "        print(list(combined_df.columns))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    combine_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a2013-4439-4a3c-9da4-11efc72c6f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afe6c2-3aa6-4dda-9c7e-05625315573a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d2f66-bcc4-4300-8797-f3697b46d07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
